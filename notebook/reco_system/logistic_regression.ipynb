{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import relevant libraries\n",
    "please remove libraries that u r not using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from statistics import median\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster but less thorough version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'clean_train_reco.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*ConvergenceWarning.*\")\n",
    "\n",
    "# Separate features and target variables\n",
    "prediction_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "X = df.drop(columns=prediction_columns)  # First 86 columns (features)\n",
    "y = df[prediction_columns]\n",
    "\n",
    "# Normalize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for .......\n",
    "Here we use logistic regression ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for model evaluation\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_models(self, X_train, y_train):\n",
    "        # Define the logistic regression model\n",
    "        lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
    "\n",
    "        # Set up hyperparameters for tuning\n",
    "        param_dist = {\n",
    "            'C': [0.001, 0.01, 0.1, 1],  # Reduced number of hyperparameters\n",
    "            'penalty': ['l1', 'l2'],  # Only two regularization types\n",
    "        }\n",
    "\n",
    "        # Use StratifiedKFold for cross-validation\n",
    "        skf = StratifiedKFold(n_splits=3)  # Reduced number of folds\n",
    "\n",
    "        # Perform random search with cross-validation\n",
    "        random_search = RandomizedSearchCV(lr_model, param_dist, n_iter=8, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "        for i in range(y_train.shape[1]):\n",
    "            print(f\"Training model for target column {y_train.columns[i]}\")\n",
    "\n",
    "            # Train the model\n",
    "            random_search.fit(X_train, y_train.iloc[:, i])\n",
    "            self.models[y_train.columns[i]] = random_search.best_estimator_\n",
    "            print(f\"Best parameters for {y_train.columns[i]}: {random_search.best_params_}\")\n",
    "\n",
    "        return self.models\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, column_name):\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(y_pred)\n",
    "        print(f\"Confusion Matrix for {column_name}:\\n\", confusion_matrix(y_test[column_name], y_pred))\n",
    "        print(f\"Classification Report for {column_name}:\\n\", classification_report(y_test[column_name], y_pred))\n",
    "        \n",
    "    def evaluate_all_models(self, X_test, y_test):\n",
    "        for column_name, model in self.models.items():\n",
    "            self.evaluate_model(model, X_test, y_test, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for target column fixed_deposits\n",
      "Best parameters for fixed_deposits: {'penalty': 'l1', 'C': 0.1}\n",
      "Training model for target column loan\n",
      "Best parameters for loan: {'penalty': 'l2', 'C': 1}\n",
      "Training model for target column credit_card_debit_card\n",
      "Best parameters for credit_card_debit_card: {'penalty': 'l2', 'C': 1}\n",
      "Training model for target column account\n",
      "Best parameters for account: {'penalty': 'l2', 'C': 1}\n",
      "[0 0 0 ... 1 0 0]\n",
      "Confusion Matrix for fixed_deposits:\n",
      " [[15177  5403]\n",
      " [  315  3105]]\n",
      "Classification Report for fixed_deposits:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.74      0.84     20580\n",
      "           1       0.36      0.91      0.52      3420\n",
      "\n",
      "    accuracy                           0.76     24000\n",
      "   macro avg       0.67      0.82      0.68     24000\n",
      "weighted avg       0.89      0.76      0.80     24000\n",
      "\n",
      "[0 0 0 ... 1 0 0]\n",
      "Confusion Matrix for loan:\n",
      " [[14512  6454]\n",
      " [  196  2838]]\n",
      "Classification Report for loan:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.69      0.81     20966\n",
      "           1       0.31      0.94      0.46      3034\n",
      "\n",
      "    accuracy                           0.72     24000\n",
      "   macro avg       0.65      0.81      0.64     24000\n",
      "weighted avg       0.90      0.72      0.77     24000\n",
      "\n",
      "[1 1 0 ... 1 0 0]\n",
      "Confusion Matrix for credit_card_debit_card:\n",
      " [[10931  7148]\n",
      " [  536  5385]]\n",
      "Classification Report for credit_card_debit_card:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.60      0.74     18079\n",
      "           1       0.43      0.91      0.58      5921\n",
      "\n",
      "    accuracy                           0.68     24000\n",
      "   macro avg       0.69      0.76      0.66     24000\n",
      "weighted avg       0.82      0.68      0.70     24000\n",
      "\n",
      "[1 1 1 ... 0 1 1]\n",
      "Confusion Matrix for account:\n",
      " [[ 3565  1032]\n",
      " [ 4356 15047]]\n",
      "Classification Report for account:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.78      0.57      4597\n",
      "           1       0.94      0.78      0.85     19403\n",
      "\n",
      "    accuracy                           0.78     24000\n",
      "   macro avg       0.69      0.78      0.71     24000\n",
      "weighted avg       0.84      0.78      0.79     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the models\n",
    "model_evaluator = ModelEvaluator()\n",
    "models = model_evaluator.train_models(X_train, y_train)\n",
    "\n",
    "# Evaluate all trained models\n",
    "model_evaluator.evaluate_all_models(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination of what the function below is for, and give sample usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(new_data):\n",
    "    probabilities = {}\n",
    "    for column_name, model in model_evaluator.models.items():\n",
    "        probabilities[column_name] = model.predict_proba(new_data)[:, 1]  # Probability of class 1\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can give an example of how predict_proba() is used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model takes a while to run, its the more thorough version. Select the code and command + / for mac to undo the # . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target variables\n",
    "# prediction_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "# X = df.drop(columns=prediction_columns)  # First 86 columns (features)\n",
    "# y = df[prediction_columns]\n",
    "\n",
    "# # Normalize the feature columns\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# # Should use, stratified train-test split to maintain class distribution, but errors encountered ()\n",
    "# # one of the classes is has too few rows\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a class for model evaluation\n",
    "# class ModelEvaluator:\n",
    "#     def __init__(self):\n",
    "#         self.models = {}\n",
    "        \n",
    "#     def train_models(self, X_train, y_train):\n",
    "#         # Define the logistic regression model\n",
    "#         lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "#         # Set up hyperparameters for tuning\n",
    "#         param_grid = {\n",
    "#             'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "#             'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type\n",
    "#             'solver': ['liblinear', 'saga']  # Solver to use\n",
    "#         }\n",
    "\n",
    "#         # Use StratifiedKFold for cross-validation\n",
    "#         skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#         # Perform grid search with cross-validation\n",
    "#         grid_search = GridSearchCV(lr_model, param_grid, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "        \n",
    "#         for i in range(y_train.shape[1]):\n",
    "#             print(f\"Training model for target column {y_train.columns[i]}\")\n",
    "#             # Train the model\n",
    "#             grid_search.fit(X_train, y_train.iloc[:, i])\n",
    "#             self.models[y_train.columns[i]] = grid_search.best_estimator_\n",
    "#             print(f\"Best parameters for {y_train.columns[i]}: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "\n",
    "#     def evaluate_model(self, model, X_test, y_test, column_name):\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         print(f\"Confusion Matrix for {column_name}:\\n\", confusion_matrix(y_test[column_name], y_pred))\n",
    "#         print(f\"Classification Report for {column_name}:\\n\", classification_report(y_test[column_name], y_pred))\n",
    "        \n",
    "#     def evaluate_all_models(self, X_test, y_test):\n",
    "#         for column_name, model in self.models.items():\n",
    "#             self.evaluate_model(model, X_test, y_test, column_name)\n",
    "\n",
    "# # Create and train the models\n",
    "# model_evaluator = ModelEvaluator()\n",
    "# model_evaluator.train_models(X_train, y_train)\n",
    "\n",
    "# # Evaluate all trained models\n",
    "# model_evaluator.evaluate_all_models(X_test, y_test)\n",
    "\n",
    "# # After training, you can use the models to make predictions on new data\n",
    "# def predict_proba(new_data):\n",
    "#     new_data_normalized = scaler.transform(new_data)\n",
    "#     probabilities = {}\n",
    "#     for column_name, model in model_evaluator.models.items():\n",
    "#         probabilities[column_name] = model.predict_proba(new_data_normalized)[:, 1]  # Probability of class 1\n",
    "#     return probabilities\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
