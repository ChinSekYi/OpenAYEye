{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from statistics import median\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster but less thorough version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for target column fixed_deposits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinsng/anaconda3/envs/dsa3101/lib/python3.9/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for fixed_deposits: {'penalty': 'l1', 'C': 0.1}\n",
      "Training model for target column loan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinsng/anaconda3/envs/dsa3101/lib/python3.9/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for loan: {'penalty': 'l1', 'C': 1}\n",
      "Training model for target column credit_card_debit_card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinsng/anaconda3/envs/dsa3101/lib/python3.9/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for credit_card_debit_card: {'penalty': 'l2', 'C': 1}\n",
      "Training model for target column account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinsng/anaconda3/envs/dsa3101/lib/python3.9/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for account: {'penalty': 'l1', 'C': 0.001}\n",
      "Confusion Matrix for fixed_deposits:\n",
      " [[1821  155]\n",
      " [   3   11]]\n",
      "Classification Report for fixed_deposits:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      1976\n",
      "           1       0.07      0.79      0.12        14\n",
      "\n",
      "    accuracy                           0.92      1990\n",
      "   macro avg       0.53      0.85      0.54      1990\n",
      "weighted avg       0.99      0.92      0.95      1990\n",
      "\n",
      "Confusion Matrix for loan:\n",
      " [[1930   58]\n",
      " [   2    0]]\n",
      "Classification Report for loan:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      1988\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97      1990\n",
      "   macro avg       0.50      0.49      0.49      1990\n",
      "weighted avg       1.00      0.97      0.98      1990\n",
      "\n",
      "Confusion Matrix for credit_card_debit_card:\n",
      " [[1377  447]\n",
      " [  21  145]]\n",
      "Classification Report for credit_card_debit_card:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.75      0.85      1824\n",
      "           1       0.24      0.87      0.38       166\n",
      "\n",
      "    accuracy                           0.76      1990\n",
      "   macro avg       0.61      0.81      0.62      1990\n",
      "weighted avg       0.92      0.76      0.82      1990\n",
      "\n",
      "Confusion Matrix for account:\n",
      " [[   9   18]\n",
      " [ 284 1679]]\n",
      "Classification Report for account:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.33      0.06        27\n",
      "           1       0.99      0.86      0.92      1963\n",
      "\n",
      "    accuracy                           0.85      1990\n",
      "   macro avg       0.51      0.59      0.49      1990\n",
      "weighted avg       0.98      0.85      0.91      1990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'renamed_reco_train.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*ConvergenceWarning.*\")\n",
    "\n",
    "# Separate features and target variables\n",
    "prediction_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "X = df.drop(columns=prediction_columns)  # First 86 columns (features)\n",
    "y = df[prediction_columns]\n",
    "\n",
    "# Normalize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a class for model evaluation\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_models(self, X_train, y_train):\n",
    "        # Define the logistic regression model\n",
    "        lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
    "\n",
    "        # Set up hyperparameters for tuning\n",
    "        param_dist = {\n",
    "            'C': [0.001, 0.01, 0.1, 1],  # Reduced number of hyperparameters\n",
    "            'penalty': ['l1', 'l2'],  # Only two regularization types\n",
    "        }\n",
    "\n",
    "        # Use StratifiedKFold for cross-validation\n",
    "        skf = StratifiedKFold(n_splits=3)  # Reduced number of folds\n",
    "\n",
    "        # Perform random search with cross-validation\n",
    "        random_search = RandomizedSearchCV(lr_model, param_dist, n_iter=10, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "        for i in range(y_train.shape[1]):\n",
    "            print(f\"Training model for target column {y_train.columns[i]}\")\n",
    "            # Train the model\n",
    "            random_search.fit(X_train, y_train.iloc[:, i])\n",
    "            self.models[y_train.columns[i]] = random_search.best_estimator_\n",
    "            print(f\"Best parameters for {y_train.columns[i]}: {random_search.best_params_}\")\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, column_name):\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"Confusion Matrix for {column_name}:\\n\", confusion_matrix(y_test[column_name], y_pred))\n",
    "        print(f\"Classification Report for {column_name}:\\n\", classification_report(y_test[column_name], y_pred))\n",
    "        \n",
    "    def evaluate_all_models(self, X_test, y_test):\n",
    "        for column_name, model in self.models.items():\n",
    "            self.evaluate_model(model, X_test, y_test, column_name)\n",
    "\n",
    "# Create and train the models\n",
    "model_evaluator = ModelEvaluator()\n",
    "model_evaluator.train_models(X_train, y_train)\n",
    "\n",
    "# Evaluate all trained models\n",
    "model_evaluator.evaluate_all_models(X_test, y_test)\n",
    "\n",
    "# After training, you can use the models to make predictions on new data\n",
    "def predict_proba(new_data):\n",
    "    new_data_normalized = scaler.transform(new_data)\n",
    "    probabilities = {}\n",
    "    for column_name, model in model_evaluator.models.items():\n",
    "        probabilities[column_name] = model.predict_proba(new_data_normalized)[:, 1]  # Probability of class 1\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model takes a while to run, its the more thorough version. Select the code and command + / for mac to undo the # . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target variables\n",
    "# prediction_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "# X = df.drop(columns=prediction_columns)  # First 86 columns (features)\n",
    "# y = df[prediction_columns]\n",
    "\n",
    "# # Normalize the feature columns\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# # Should use, stratified train-test split to maintain class distribution, but errors encountered ()\n",
    "# # one of the classes is has too few rows\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a class for model evaluation\n",
    "# class ModelEvaluator:\n",
    "#     def __init__(self):\n",
    "#         self.models = {}\n",
    "        \n",
    "#     def train_models(self, X_train, y_train):\n",
    "#         # Define the logistic regression model\n",
    "#         lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "#         # Set up hyperparameters for tuning\n",
    "#         param_grid = {\n",
    "#             'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "#             'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type\n",
    "#             'solver': ['liblinear', 'saga']  # Solver to use\n",
    "#         }\n",
    "\n",
    "#         # Use StratifiedKFold for cross-validation\n",
    "#         skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#         # Perform grid search with cross-validation\n",
    "#         grid_search = GridSearchCV(lr_model, param_grid, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "        \n",
    "#         for i in range(y_train.shape[1]):\n",
    "#             print(f\"Training model for target column {y_train.columns[i]}\")\n",
    "#             # Train the model\n",
    "#             grid_search.fit(X_train, y_train.iloc[:, i])\n",
    "#             self.models[y_train.columns[i]] = grid_search.best_estimator_\n",
    "#             print(f\"Best parameters for {y_train.columns[i]}: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "\n",
    "#     def evaluate_model(self, model, X_test, y_test, column_name):\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         print(f\"Confusion Matrix for {column_name}:\\n\", confusion_matrix(y_test[column_name], y_pred))\n",
    "#         print(f\"Classification Report for {column_name}:\\n\", classification_report(y_test[column_name], y_pred))\n",
    "        \n",
    "#     def evaluate_all_models(self, X_test, y_test):\n",
    "#         for column_name, model in self.models.items():\n",
    "#             self.evaluate_model(model, X_test, y_test, column_name)\n",
    "\n",
    "# # Create and train the models\n",
    "# model_evaluator = ModelEvaluator()\n",
    "# model_evaluator.train_models(X_train, y_train)\n",
    "\n",
    "# # Evaluate all trained models\n",
    "# model_evaluator.evaluate_all_models(X_test, y_test)\n",
    "\n",
    "# # After training, you can use the models to make predictions on new data\n",
    "# def predict_proba(new_data):\n",
    "#     new_data_normalized = scaler.transform(new_data)\n",
    "#     probabilities = {}\n",
    "#     for column_name, model in model_evaluator.models.items():\n",
    "#         probabilities[column_name] = model.predict_proba(new_data_normalized)[:, 1]  # Probability of class 1\n",
    "#     return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
