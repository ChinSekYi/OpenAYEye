{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from statistics import median\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster but less thorough version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'clean_train_reco.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*ConvergenceWarning.*\")\n",
    "\n",
    "# Separate features and target variables\n",
    "prediction_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "X = df.drop(columns=prediction_columns)  # First 86 columns (features)\n",
    "y = df[prediction_columns]\n",
    "\n",
    "# Normalize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for model evaluation\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_models(self, X_train, y_train):\n",
    "        # Define the logistic regression model\n",
    "        lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
    "\n",
    "        # Set up hyperparameters for tuning\n",
    "        param_dist = {\n",
    "            'C': [0.001, 0.01, 0.1, 1],  # Reduced number of hyperparameters\n",
    "            'penalty': ['l1', 'l2'],  # Only two regularization types\n",
    "        }\n",
    "\n",
    "        # Use StratifiedKFold for cross-validation\n",
    "        skf = StratifiedKFold(n_splits=3)  # Reduced number of folds\n",
    "\n",
    "        # Perform random search with cross-validation\n",
    "        random_search = RandomizedSearchCV(lr_model, param_dist, n_iter=8, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "        for i in range(y_train.shape[1]):\n",
    "            print(f\"Training model for target column {y_train.columns[i]}\")\n",
    "            # Train the model\n",
    "            random_search.fit(X_train, y_train.iloc[:, i])\n",
    "            self.models[y_train.columns[i]] = random_search.best_estimator_\n",
    "            print(f\"Best parameters for {y_train.columns[i]}: {random_search.best_params_}\")\n",
    "\n",
    "        return self.models\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, column_name):\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"Confusion Matrix for {column_name}:\\n\", confusion_matrix(y_test[column_name], y_pred))\n",
    "        print(f\"Classification Report for {column_name}:\\n\", classification_report(y_test[column_name], y_pred))\n",
    "        \n",
    "    def evaluate_all_models(self, X_test, y_test):\n",
    "        for column_name, model in self.models.items():\n",
    "            self.evaluate_model(model, X_test, y_test, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         fixed_deposits  loan  credit_card_debit_card  account\n",
       "0                    0     0                       0        1\n",
       "1                    0     0                       0        1\n",
       "2                    0     1                       0        1\n",
       "3                    0     0                       0        1\n",
       "4                    1     0                       0        1\n",
       "...                ...   ...                     ...      ...\n",
       "119995               0     0                       0        0\n",
       "119996               0     0                       0        0\n",
       "119997               0     0                       0        1\n",
       "119998               0     0                       0        1\n",
       "119999               1     0                       1        1\n",
       "\n",
       "[120000 rows x 4 columns]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for target column fixed_deposits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlychinsekyi/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for fixed_deposits: {'penalty': 'l1', 'C': 1}\n",
      "Training model for target column loan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlychinsekyi/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for loan: {'penalty': 'l2', 'C': 1}\n",
      "Training model for target column credit_card_debit_card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlychinsekyi/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for credit_card_debit_card: {'penalty': 'l2', 'C': 1}\n",
      "Training model for target column account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlychinsekyi/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for account: {'penalty': 'l1', 'C': 1}\n",
      "Confusion Matrix for fixed_deposits:\n",
      " [[15069  5487]\n",
      " [  315  3129]]\n",
      "Classification Report for fixed_deposits:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.73      0.84     20556\n",
      "           1       0.36      0.91      0.52      3444\n",
      "\n",
      "    accuracy                           0.76     24000\n",
      "   macro avg       0.67      0.82      0.68     24000\n",
      "weighted avg       0.89      0.76      0.79     24000\n",
      "\n",
      "Confusion Matrix for loan:\n",
      " [[13817  7132]\n",
      " [  114  2937]]\n",
      "Classification Report for loan:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.66      0.79     20949\n",
      "           1       0.29      0.96      0.45      3051\n",
      "\n",
      "    accuracy                           0.70     24000\n",
      "   macro avg       0.64      0.81      0.62     24000\n",
      "weighted avg       0.90      0.70      0.75     24000\n",
      "\n",
      "Confusion Matrix for credit_card_debit_card:\n",
      " [[10375  7812]\n",
      " [  580  5233]]\n",
      "Classification Report for credit_card_debit_card:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.57      0.71     18187\n",
      "           1       0.40      0.90      0.55      5813\n",
      "\n",
      "    accuracy                           0.65     24000\n",
      "   macro avg       0.67      0.74      0.63     24000\n",
      "weighted avg       0.81      0.65      0.67     24000\n",
      "\n",
      "Confusion Matrix for account:\n",
      " [[ 3691   975]\n",
      " [ 6092 13242]]\n",
      "Classification Report for account:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.79      0.51      4666\n",
      "           1       0.93      0.68      0.79     19334\n",
      "\n",
      "    accuracy                           0.71     24000\n",
      "   macro avg       0.65      0.74      0.65     24000\n",
      "weighted avg       0.82      0.71      0.74     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the models\n",
    "model_evaluator = ModelEvaluator()\n",
    "models = model_evaluator.train_models(X_train, y_train)\n",
    "\n",
    "# Evaluate all trained models\n",
    "model_evaluator.evaluate_all_models(X_test, y_test)\n",
    "\n",
    "# After training, you can use the models to make predictions on new data\n",
    "def predict_proba(new_data):\n",
    "    new_data_normalized = scaler.transform(new_data)\n",
    "    probabilities = {}\n",
    "    for column_name, model in model_evaluator.models.items():\n",
    "        probabilities[column_name] = model.predict_proba(new_data_normalized)[:, 1]  # Probability of class 1\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fixed_deposits': LogisticRegression(C=1, class_weight='balanced', max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear'),\n",
       " 'loan': LogisticRegression(C=1, class_weight='balanced', max_iter=1000,\n",
       "                    solver='liblinear'),\n",
       " 'credit_card_debit_card': LogisticRegression(C=1, class_weight='balanced', max_iter=1000,\n",
       "                    solver='liblinear'),\n",
       " 'account': LogisticRegression(C=1, class_weight='balanced', max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model takes a while to run, its the more thorough version. Select the code and command + / for mac to undo the # . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target variables\n",
    "# prediction_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "# X = df.drop(columns=prediction_columns)  # First 86 columns (features)\n",
    "# y = df[prediction_columns]\n",
    "\n",
    "# # Normalize the feature columns\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# # Should use, stratified train-test split to maintain class distribution, but errors encountered ()\n",
    "# # one of the classes is has too few rows\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a class for model evaluation\n",
    "# class ModelEvaluator:\n",
    "#     def __init__(self):\n",
    "#         self.models = {}\n",
    "        \n",
    "#     def train_models(self, X_train, y_train):\n",
    "#         # Define the logistic regression model\n",
    "#         lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "#         # Set up hyperparameters for tuning\n",
    "#         param_grid = {\n",
    "#             'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "#             'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type\n",
    "#             'solver': ['liblinear', 'saga']  # Solver to use\n",
    "#         }\n",
    "\n",
    "#         # Use StratifiedKFold for cross-validation\n",
    "#         skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#         # Perform grid search with cross-validation\n",
    "#         grid_search = GridSearchCV(lr_model, param_grid, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "        \n",
    "#         for i in range(y_train.shape[1]):\n",
    "#             print(f\"Training model for target column {y_train.columns[i]}\")\n",
    "#             # Train the model\n",
    "#             grid_search.fit(X_train, y_train.iloc[:, i])\n",
    "#             self.models[y_train.columns[i]] = grid_search.best_estimator_\n",
    "#             print(f\"Best parameters for {y_train.columns[i]}: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "\n",
    "#     def evaluate_model(self, model, X_test, y_test, column_name):\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         print(f\"Confusion Matrix for {column_name}:\\n\", confusion_matrix(y_test[column_name], y_pred))\n",
    "#         print(f\"Classification Report for {column_name}:\\n\", classification_report(y_test[column_name], y_pred))\n",
    "        \n",
    "#     def evaluate_all_models(self, X_test, y_test):\n",
    "#         for column_name, model in self.models.items():\n",
    "#             self.evaluate_model(model, X_test, y_test, column_name)\n",
    "\n",
    "# # Create and train the models\n",
    "# model_evaluator = ModelEvaluator()\n",
    "# model_evaluator.train_models(X_train, y_train)\n",
    "\n",
    "# # Evaluate all trained models\n",
    "# model_evaluator.evaluate_all_models(X_test, y_test)\n",
    "\n",
    "# # After training, you can use the models to make predictions on new data\n",
    "# def predict_proba(new_data):\n",
    "#     new_data_normalized = scaler.transform(new_data)\n",
    "#     probabilities = {}\n",
    "#     for column_name, model in model_evaluator.models.items():\n",
    "#         probabilities[column_name] = model.predict_proba(new_data_normalized)[:, 1]  # Probability of class 1\n",
    "#     return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
