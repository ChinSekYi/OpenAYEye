{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and EDA for Santender dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns of the csv file to english readeable names, in addition the dataset used here is a subset of the original dataset, the original dataset train is 13,647,000 rows and test is 900,000 rows, loading it in will crash your computer. \n",
    "Currently the train file in spanish is named sandenter_train_small and test is sandenter_test_small. The code uses sandenter_train_small as it has enough rows to do both train and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries\n",
    "Please remove libraries that youre not using....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from statistics import median\n",
    "from imblearn.over_sampling import SMOTE #Note to add this to requirements.txt, conda install -c conda-forge imbalanced-learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the names of the files to clean and the name of the cleaned files here:\n",
    "old_file = \"recodataset.csv\"\n",
    "clean_train = \"clean_train_reco.csv\"\n",
    "clean_test = \"clean_test_reco.csv\"\n",
    "\n",
    "number_of_each_class = 15000 #Change according to desired size of original dataset, \n",
    "#final dataset size is this number * number of products * 2 because binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = old_file  # input CSV file\n",
    "output_file_path = clean_train  # output training CSV file\n",
    "\n",
    "# Define the mapping of original Spanish column names to English column names\n",
    "column_mapping = {\n",
    "    \"fecha_dato\": \"report_date\",\n",
    "    \"ncodpers\": \"customer_id\",\n",
    "    \"ind_empleado\": \"employee_index\",\n",
    "    \"pais_residencia\": \"country_residence\",\n",
    "    \"sexo\": \"gender\",\n",
    "    \"age\": \"age\",\n",
    "    \"fecha_alta\": \"contract_start_date\",\n",
    "    \"ind_nuevo\": \"new_customer_index\",\n",
    "    \"antiguedad\": \"seniority_months\",\n",
    "    \"indrel\": \"primary_customer_status\",\n",
    "    \"ult_fec_cli_1t\": \"last_primary_customer_date\",\n",
    "    \"indrel_1mes\": \"customer_type_start_month\",\n",
    "    \"tiprel_1mes\": \"customer_relation_type\",\n",
    "    \"indresi\": \"residence_index\",\n",
    "    \"indext\": \"foreigner_index\",\n",
    "    \"conyuemp\": \"spouse_employee_index\",\n",
    "    \"canal_entrada\": \"join_channel\",\n",
    "    \"indfall\": \"deceased_index\",\n",
    "    \"tipodom\": \"address_type\",\n",
    "    \"cod_prov\": \"province_code\",\n",
    "    \"nomprov\": \"province_name\",\n",
    "    \"ind_actividad_cliente\": \"activity_index\",\n",
    "    \"renta\": \"gross_income\",\n",
    "    \"segmento\": \"customer_segment\",\n",
    "    \"ind_ahor_fin_ult1\": \"saving_account\",\n",
    "    \"ind_aval_fin_ult1\": \"guarantee\",\n",
    "    \"ind_cco_fin_ult1\": \"current_account\",\n",
    "    \"ind_cder_fin_ult1\": \"derivada_account\",\n",
    "    \"ind_cno_fin_ult1\": \"payroll_account\",\n",
    "    \"ind_ctju_fin_ult1\": \"junior_account\",\n",
    "    \"ind_ctma_fin_ult1\": \"more_particular_account\",\n",
    "    \"ind_ctop_fin_ult1\": \"particular_account\",\n",
    "    \"ind_ctpp_fin_ult1\": \"particular_plus_account\",\n",
    "    \"ind_deco_fin_ult1\": \"short_term_deposits\",\n",
    "    \"ind_deme_fin_ult1\": \"medium_term_deposits\",\n",
    "    \"ind_dela_fin_ult1\": \"long_term_deposits\",\n",
    "    \"ind_ecue_fin_ult1\": \"e_account\",\n",
    "    \"ind_fond_fin_ult1\": \"funds\",\n",
    "    \"ind_hip_fin_ult1\": \"mortgage\",\n",
    "    \"ind_plan_fin_ult1\": \"pensions\",\n",
    "    \"ind_pres_fin_ult1\": \"loans\",\n",
    "    \"ind_reca_fin_ult1\": \"taxes\",\n",
    "    \"ind_tjcr_fin_ult1\": \"credit_card\",\n",
    "    \"ind_valo_fin_ult1\": \"securities\",\n",
    "    \"ind_viv_fin_ult1\": \"home_account\",\n",
    "    \"ind_nomina_ult1\": \"payroll\",\n",
    "    \"ind_nom_pens_ult1\": \"pensions_payments\",\n",
    "    \"ind_recibo_ult1\": \"direct_debit\"\n",
    "}\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "#Drop the useless columns (correlation matrix used https://medium.com/@samarthjoelram/santander-recommendation-system-cab6b40596b5)\n",
    "\n",
    "# List of columns to drop if they exist\n",
    "columns_to_drop = ['ult_fec_cli_1t', 'ind_actividad_cliente', 'cod_prov', 'conyuemp', 'tipodom']\n",
    "\n",
    "# Drop columns if they exist in the DataFrame\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Rename the columns to english\n",
    "df.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regrouping bank products into 4 main categories\n",
    "- fixed-deposits\n",
    "- loan\n",
    "- accounts\n",
    "- credit and debit cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new column names\n",
    "fixed_deposits_col = 'fixed_deposits'\n",
    "loan_col = 'loan'\n",
    "credit_card_debit_card_col = 'credit_card_debit_card'\n",
    "account_col = 'account'\n",
    "\n",
    "# Check and create a new column for fixed deposits, if it doesn't exist\n",
    "if fixed_deposits_col not in df.columns:\n",
    "    deposit_columns = [\n",
    "        \"short_term_deposits\",  # ind_deco_fin_ult1\n",
    "        \"medium_term_deposits\",  # ind_deme_fin_ult1\n",
    "        \"long_term_deposits\"    # ind_dela_fin_ult1\n",
    "    ]\n",
    "    df[fixed_deposits_col] = df[deposit_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Check and create a new column for loans, if it doesn't exist\n",
    "if loan_col not in df.columns:\n",
    "    loan_columns = [\n",
    "        \"loans\",                # ind_pres_fin_ult1\n",
    "        \"pensions\"             # ind_plan_fin_ult1\n",
    "    ]\n",
    "    df[loan_col] = df[loan_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Check and create a new column for credit and debit cards, if it doesn't exist\n",
    "if credit_card_debit_card_col not in df.columns:\n",
    "    credit_card_columns = [\n",
    "        \"credit_card\",         # ind_tjcr_fin_ult1\n",
    "        \"direct_debit\"        # ind_recibo_ult1\n",
    "    ]\n",
    "    df[credit_card_debit_card_col] = df[credit_card_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Check and create a new column for all accounts combined, if it doesn't exist\n",
    "if account_col not in df.columns:\n",
    "    account_columns = [\n",
    "        \"saving_account\",      # ind_ahor_fin_ult1\n",
    "        \"current_account\",     # ind_cco_fin_ult1\n",
    "        \"derivada_account\",    # ind_cder_fin_ult1\n",
    "        \"payroll_account\",     # ind_cno_fin_ult1\n",
    "        \"junior_account\",      # ind_ctju_fin_ult1\n",
    "        \"more_particular_account\",  # ind_ctma_fin_ult1\n",
    "        \"particular_account\",   # ind_ctop_fin_ult1\n",
    "        \"particular_plus_account\", # ind_ctpp_fin_ult1\n",
    "        \"e_account\",           # ind_ecue_fin_ult1\n",
    "        \"funds\",               # ind_fond_fin_ult1\n",
    "        \"home_account\",        # ind_viv_fin_ult1\n",
    "    ]\n",
    "    df[account_col] = df[account_columns].any(axis=1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnescessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model used for commercial purposes does not merge the products together\n",
    "columns_to_drop = [\n",
    "    'saving_account', 'guarantee', 'current_account', 'derivada_account', 'payroll_account', \n",
    "    'junior_account', 'more_particular_account', 'particular_account', 'particular_plus_account', \n",
    "    'short_term_deposits', 'medium_term_deposits', 'long_term_deposits', 'e_account', 'funds', \n",
    "    'mortgage', 'pensions', 'loans', 'taxes', 'credit_card', 'securities', 'home_account', \n",
    "    'payroll', 'pensions_payments', 'direct_debit', 'employee_index'\n",
    "]\n",
    "\n",
    "# Drop the columns if they exist\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation and Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['report_date', 'contract_start_date']  # Add any other date columns if necessary\n",
    "\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Calculate the difference in days\n",
    "df['contract_length'] = (df['report_date'] - df['contract_start_date']).dt.days\n",
    "\n",
    "# Insert 'contract_length' in the same spot as 'contract_start_date'\n",
    "start_date_index = df.columns.get_loc('contract_start_date')\n",
    "df.insert(start_date_index, 'contract_length', df.pop('contract_length'))\n",
    "\n",
    "# Drop the original 'contract_start_date' and 'report_date' columns\n",
    "df = df.drop(['contract_start_date', 'report_date', 'customer_id'], axis='columns')\n",
    "\n",
    "# Replace missing values in gross income and age with median of distribution\n",
    "count = df['gross_income'].isna().sum()\n",
    "df['gross_income'] = df['gross_income'].fillna(df['gross_income'].median())\n",
    "\n",
    "\n",
    "df['age'] = pd.to_numeric(df['age'].replace(' NA', None), errors='coerce')\n",
    "med_age = df['age'].median()\n",
    "df['age'] = df['age'].fillna(med_age).astype(int)\n",
    "\n",
    "# Specify the columns to check for NA values\n",
    "columns_with_na = [\n",
    "    'country_residence', 'gender', \n",
    "    'customer_relation_type', 'residence_index', \n",
    "    'foreigner_index', 'join_channel', 'deceased_index', \n",
    "    'customer_segment', 'new_customer_index'\n",
    "]\n",
    "\n",
    "# Drop rows with NA values in the specified columns\n",
    "df = df.dropna(subset=columns_with_na)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip leading and trailing spaces from string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_spaces(column):\n",
    "    if column.dtype == 'object':  # Check if the column is of string type\n",
    "        return column.str.strip()  # Strip leading and trailing spaces\n",
    "    return column\n",
    "\n",
    "# Apply the strip_spaces function to all columns in the DataFrame\n",
    "df = df.apply(strip_spaces)\n",
    "\n",
    "# Remove leading/trailing whitespace of seniority_months column and convert to numeric\n",
    "df['seniority_months'] = pd.to_numeric(df['seniority_months'].str.strip(), errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regrouping province into 5 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing province name to regions so less columns created via one hot encoding\n",
    "region = []\n",
    "for province in df['province_name']:\n",
    "    if province in ['CIUDAD REAL', 'SALAMANCA', 'TOLEDO', 'SEGOVIA', 'MADRID', 'GUADALAJARA', 'ALBACETE', 'SORIA', 'CUENCA', 'AVILA']:\n",
    "        region.append(\"CENTRAL\")\n",
    "    elif province in ['ALAVA', 'GIPUZKOA', 'PALENCIA', 'BURGOS', 'NAVARRA', 'CANTABRIA', 'BIZKAIA', 'RIOJA, LA', 'ZARAGOZA', 'TARRAGONA', 'LERIDA', 'HUESCA']:\n",
    "        region.append(\"NORTH\")\n",
    "    elif province in ['CADIZ', 'JAEN', 'SEVILLA', 'PALMAS, LAS', 'CORDOBA', 'GRANADA', 'SANTA CRUZ DE TENERIFE', 'MELILLA', 'CEUTA', 'MALAGA']:\n",
    "        region.append(\"SOUTH\")\n",
    "    elif province in ['VALENCIA', 'TERUEL', 'BALEARS, ILLES', 'CASTELLON', 'ALICANTE', 'MURCIA', 'ALMERIA', 'BARCELONA', 'GIRONA']:\n",
    "        region.append(\"EAST\")\n",
    "    elif province in ['ZAMORA', 'CACERES', 'HUELVA', 'BADAJOZ', 'ASTURIAS', 'LEON', 'LUGO', 'CORUÑA, A', 'OURENSE', 'VALLADOLID', 'PONTEVEDRA']:\n",
    "        region.append(\"WEST\")\n",
    "    else:\n",
    "        region.append(None)  # Append None for unmatched provinces\n",
    "\n",
    "# Assign the new region list to the DataFrame\n",
    "df['region'] = region\n",
    "df = df.drop(columns=['province_name'])  # Drop the original 'province_name' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the specified columns in columns_with_na\n",
    "df = pd.get_dummies(df, columns=columns_with_na, drop_first=False)\n",
    "\n",
    "# One-hot encode the 'region' column\n",
    "if 'region' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['region'], drop_first=True)\n",
    "    \n",
    "# Convert True/False columns to 1/0\n",
    "bool_columns = df.select_dtypes(include=['bool']).columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Perform train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the 80/20 train-test split\n",
    "df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset indices for both datasets to avoid misaligned indexing\n",
    "df = df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Save the test dataset to CSV\n",
    "test_df.to_csv(\"clean_test_reco.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform SMOTE\n",
    "Rationale:\n",
    "- Solve the problem of imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing for label: fixed_deposits\n",
      "Balancing for label: loan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/k1hh45tx53v5c4hp65ntyjtc0000gn/T/ipykernel_12302/2999078696.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  resampled_df[label] = y_resampled  # Add the resampled label back\n",
      "/var/folders/pg/k1hh45tx53v5c4hp65ntyjtc0000gn/T/ipykernel_12302/2999078696.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  resampled_df[label] = y_resampled  # Add the resampled label back\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing for label: credit_card_debit_card\n",
      "Balancing for label: account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/k1hh45tx53v5c4hp65ntyjtc0000gn/T/ipykernel_12302/2999078696.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  resampled_df[label] = y_resampled  # Add the resampled label back\n",
      "/var/folders/pg/k1hh45tx53v5c4hp65ntyjtc0000gn/T/ipykernel_12302/2999078696.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  resampled_df[label] = y_resampled  # Add the resampled label back\n"
     ]
    }
   ],
   "source": [
    "#select an equal amount of each class, and use SMOTE to balance the rest\n",
    "\n",
    "# Initialize the new balanced DataFrame\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Define the label columns to balance\n",
    "label_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "\n",
    "# Set the maximum number of samples to take from any class\n",
    "max_samples = number_of_each_class  # Adjust as needed\n",
    "\n",
    "# Loop over each label column to balance it individually\n",
    "for label in label_columns:\n",
    "    print(f\"Balancing for label: {label}\")\n",
    "\n",
    "    # Separate the current label and the features\n",
    "    y = df[label]\n",
    "    X = df.drop(columns=label_columns)  # Keep all features but exclude other labels\n",
    "\n",
    "    # Prepare the data to balance the 0 and 1 classes for the current label\n",
    "    class_0 = df[df[label] == 0]\n",
    "    class_1 = df[df[label] == 1]\n",
    "\n",
    "    # Take a max of 'max_samples' or the available samples for each class\n",
    "    sampled_class_0 = class_0.sample(n=min(len(class_0), max_samples), random_state=42)\n",
    "    sampled_class_1 = class_1.sample(n=min(len(class_1), max_samples), random_state=42)\n",
    "\n",
    "    # Combine the samples to form the data for SMOTE\n",
    "    df_to_balance = pd.concat([sampled_class_0, sampled_class_1], ignore_index=True)\n",
    "\n",
    "    # Separate features and the label for SMOTE\n",
    "    X_balance = df_to_balance.drop(columns=label)\n",
    "    y_balance = df_to_balance[label]\n",
    "\n",
    "    # Apply SMOTE to balance the current label\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_balance, y_balance)\n",
    "\n",
    "    # Create a DataFrame from the resampled data\n",
    "    resampled_df = pd.DataFrame(X_resampled, columns=X_balance.columns)\n",
    "    resampled_df[label] = y_resampled  # Add the resampled label back\n",
    "\n",
    "    # Append the resampled data to the balanced_df\n",
    "    balanced_df = pd.concat([balanced_df, resampled_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully relabelled and saved to clean_train_reco.csv\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the final balanced DataFrame\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True).head(number_of_each_class*2*len(label_columns))\n",
    "\n",
    "# Define the target columns to move to the front\n",
    "columns_to_move = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "\n",
    "# Remove the target columns from the DataFrame\n",
    "remaining_columns = [col for col in balanced_df.columns if col not in columns_to_move]\n",
    "\n",
    "# Add the target columns back to the front\n",
    "balanced_df = balanced_df[columns_to_move + remaining_columns]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "balanced_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file successfully relabelled and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking class distribution for the y values\n",
    "To check if SMOTE indeed solve the class imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance for 'fixed_deposits':\n",
      "Counts:\n",
      "fixed_deposits\n",
      "0    102722\n",
      "1     17278\n",
      "Name: count, dtype: int64\n",
      "Percentages:\n",
      "fixed_deposits\n",
      "0    85.601667\n",
      "1    14.398333\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Class imbalance for 'loan':\n",
      "Counts:\n",
      "loan\n",
      "0    104874\n",
      "1     15126\n",
      "Name: count, dtype: int64\n",
      "Percentages:\n",
      "loan\n",
      "0    87.395\n",
      "1    12.605\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Class imbalance for 'credit_card_debit_card':\n",
      "Counts:\n",
      "credit_card_debit_card\n",
      "0    90461\n",
      "1    29539\n",
      "Name: count, dtype: int64\n",
      "Percentages:\n",
      "credit_card_debit_card\n",
      "0    75.384167\n",
      "1    24.615833\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Class imbalance for 'account':\n",
      "Counts:\n",
      "account\n",
      "1    96700\n",
      "0    23300\n",
      "Name: count, dtype: int64\n",
      "Percentages:\n",
      "account\n",
      "1    80.583333\n",
      "0    19.416667\n",
      "Name: count, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of columns to check for class imbalance\n",
    "columns_to_check = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "\n",
    "# Loop through each column and calculate class imbalance\n",
    "for column in columns_to_check:\n",
    "    class_counts = balanced_df[column].value_counts()\n",
    "    total_count = class_counts.sum()\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = (class_counts / total_count) * 100\n",
    "    \n",
    "    print(f\"Class imbalance for '{column}':\")\n",
    "    print(f\"Counts:\\n{class_counts}\")\n",
    "    print(f\"Percentages:\\n{percentages}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
