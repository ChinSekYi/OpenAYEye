{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns of the csv file to english readeable names, in addition the dataset used here is a subset of the original dataset, the original dataset train is 13,647,000 rows and test is 900,000 rows, loading it in will crash your computer. \n",
    "### Currently the train file in spanish is named sandenter_train_small and test is sandenter_test_small. The code uses sandenter_train_small as it has enough rows to do both train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from statistics import median\n",
    "from imblearn.over_sampling import SMOTE #Note to add this to requirements.txt, conda install -c conda-forge imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The code block below allows us to edit the settings of the code, once done hit run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the names of the files to clean and the name of the cleaned files here:\n",
    "old_file = \"recodataset.csv\"\n",
    "cleaned_file = \"cleaned_reco.csv\"\n",
    "number_of_each_class = 15000 #Change according to desired size of original dataset, \n",
    "#final dataset size is this number * number of products * 2 because binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new file to for the translated version, this is solely for visualisation purposes and is not used. \n",
    "# The original spanish verison will be used.\n",
    "\n",
    "csv_file_path = old_file  # input CSV file\n",
    "output_file_path = cleaned_file  # output CSV file\n",
    "\n",
    "# Define the mapping of original Spanish column names to English column names\n",
    "column_mapping = {\n",
    "    \"fecha_dato\": \"report_date\",\n",
    "    \"ncodpers\": \"customer_id\",\n",
    "    \"ind_empleado\": \"employee_index\",\n",
    "    \"pais_residencia\": \"country_residence\",\n",
    "    \"sexo\": \"gender\",\n",
    "    \"age\": \"age\",\n",
    "    \"fecha_alta\": \"contract_start_date\",\n",
    "    \"ind_nuevo\": \"new_customer_index\",\n",
    "    \"antiguedad\": \"seniority_months\",\n",
    "    \"indrel\": \"primary_customer_status\",\n",
    "    \"ult_fec_cli_1t\": \"last_primary_customer_date\",\n",
    "    \"indrel_1mes\": \"customer_type_start_month\",\n",
    "    \"tiprel_1mes\": \"customer_relation_type\",\n",
    "    \"indresi\": \"residence_index\",\n",
    "    \"indext\": \"foreigner_index\",\n",
    "    \"conyuemp\": \"spouse_employee_index\",\n",
    "    \"canal_entrada\": \"join_channel\",\n",
    "    \"indfall\": \"deceased_index\",\n",
    "    \"tipodom\": \"address_type\",\n",
    "    \"cod_prov\": \"province_code\",\n",
    "    \"nomprov\": \"province_name\",\n",
    "    \"ind_actividad_cliente\": \"activity_index\",\n",
    "    \"renta\": \"gross_income\",\n",
    "    \"segmento\": \"customer_segment\",\n",
    "    \"ind_ahor_fin_ult1\": \"saving_account\",\n",
    "    \"ind_aval_fin_ult1\": \"guarantee\",\n",
    "    \"ind_cco_fin_ult1\": \"current_account\",\n",
    "    \"ind_cder_fin_ult1\": \"derivada_account\",\n",
    "    \"ind_cno_fin_ult1\": \"payroll_account\",\n",
    "    \"ind_ctju_fin_ult1\": \"junior_account\",\n",
    "    \"ind_ctma_fin_ult1\": \"more_particular_account\",\n",
    "    \"ind_ctop_fin_ult1\": \"particular_account\",\n",
    "    \"ind_ctpp_fin_ult1\": \"particular_plus_account\",\n",
    "    \"ind_deco_fin_ult1\": \"short_term_deposits\",\n",
    "    \"ind_deme_fin_ult1\": \"medium_term_deposits\",\n",
    "    \"ind_dela_fin_ult1\": \"long_term_deposits\",\n",
    "    \"ind_ecue_fin_ult1\": \"e_account\",\n",
    "    \"ind_fond_fin_ult1\": \"funds\",\n",
    "    \"ind_hip_fin_ult1\": \"mortgage\",\n",
    "    \"ind_plan_fin_ult1\": \"pensions\",\n",
    "    \"ind_pres_fin_ult1\": \"loans\",\n",
    "    \"ind_reca_fin_ult1\": \"taxes\",\n",
    "    \"ind_tjcr_fin_ult1\": \"credit_card\",\n",
    "    \"ind_valo_fin_ult1\": \"securities\",\n",
    "    \"ind_viv_fin_ult1\": \"home_account\",\n",
    "    \"ind_nomina_ult1\": \"payroll\",\n",
    "    \"ind_nom_pens_ult1\": \"pensions_payments\",\n",
    "    \"ind_recibo_ult1\": \"direct_debit\"\n",
    "}\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "#Drop the useless columns (correlation matrix used https://medium.com/@samarthjoelram/santander-recommendation-system-cab6b40596b5)\n",
    "# List of columns to drop if they exist\n",
    "columns_to_drop = ['ult_fec_cli_1t', 'ind_actividad_cliente', 'cod_prov', 'conyuemp', 'tipodom']\n",
    "# Drop columns if they exist in the DataFrame\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Rename the columns to english\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Making changes to spanish datafile to suit our needs, the simplified dataset is the one we assume is collected by \n",
    "#companies, the change here is the many products have been summarised by 4 categories of products\n",
    "\n",
    "\n",
    "# Define new column names\n",
    "fixed_deposits_col = 'fixed_deposits'\n",
    "loan_col = 'loan'\n",
    "credit_card_debit_card_col = 'credit_card_debit_card'\n",
    "account_col = 'account'\n",
    "\n",
    "# Check and create a new column for fixed deposits, if it doesn't exist\n",
    "if fixed_deposits_col not in df.columns:\n",
    "    deposit_columns = [\n",
    "        \"short_term_deposits\",  # ind_deco_fin_ult1\n",
    "        \"medium_term_deposits\",  # ind_deme_fin_ult1\n",
    "        \"long_term_deposits\"    # ind_dela_fin_ult1\n",
    "    ]\n",
    "    df[fixed_deposits_col] = df[deposit_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Check and create a new column for loans, if it doesn't exist\n",
    "if loan_col not in df.columns:\n",
    "    loan_columns = [\n",
    "        \"loans\",                # ind_pres_fin_ult1\n",
    "        \"pensions\"             # ind_plan_fin_ult1\n",
    "    ]\n",
    "    df[loan_col] = df[loan_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Check and create a new column for credit and debit cards, if it doesn't exist\n",
    "if credit_card_debit_card_col not in df.columns:\n",
    "    credit_card_columns = [\n",
    "        \"credit_card\",         # ind_tjcr_fin_ult1\n",
    "        \"direct_debit\"        # ind_recibo_ult1\n",
    "    ]\n",
    "    df[credit_card_debit_card_col] = df[credit_card_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Check and create a new column for all accounts combined, if it doesn't exist\n",
    "if account_col not in df.columns:\n",
    "    account_columns = [\n",
    "        \"saving_account\",      # ind_ahor_fin_ult1\n",
    "        \"current_account\",     # ind_cco_fin_ult1\n",
    "        \"derivada_account\",    # ind_cder_fin_ult1\n",
    "        \"payroll_account\",     # ind_cno_fin_ult1\n",
    "        \"junior_account\",      # ind_ctju_fin_ult1\n",
    "        \"more_particular_account\",  # ind_ctma_fin_ult1\n",
    "        \"particular_account\",   # ind_ctop_fin_ult1\n",
    "        \"particular_plus_account\", # ind_ctpp_fin_ult1\n",
    "        \"e_account\",           # ind_ecue_fin_ult1\n",
    "        \"funds\",               # ind_fond_fin_ult1\n",
    "        \"home_account\",        # ind_viv_fin_ult1\n",
    "    ]\n",
    "    df[account_col] = df[account_columns].any(axis=1).astype(int)\n",
    "    \n",
    "# List of columns to drop, we merged these columns for to keep it simple to present \n",
    "# The model used for commercial purposes does not merge the products together\n",
    "columns_to_drop = [\n",
    "    'saving_account', 'guarantee', 'current_account', 'derivada_account', 'payroll_account', \n",
    "    'junior_account', 'more_particular_account', 'particular_account', 'particular_plus_account', \n",
    "    'short_term_deposits', 'medium_term_deposits', 'long_term_deposits', 'e_account', 'funds', \n",
    "    'mortgage', 'pensions', 'loans', 'taxes', 'credit_card', 'securities', 'home_account', \n",
    "    'payroll', 'pensions_payments', 'direct_debit'\n",
    "]\n",
    "\n",
    "# Drop the columns if they exist\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['report_date', 'contract_start_date']  # Add any other date columns if necessary\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "# Calculate the difference in days\n",
    "df['contract_length'] = (df['report_date'] - df['contract_start_date']).dt.days\n",
    "\n",
    "# Insert 'contract_length' in the same spot as 'contract_start_date'\n",
    "start_date_index = df.columns.get_loc('contract_start_date')\n",
    "df.insert(start_date_index, 'contract_length', df.pop('contract_length'))\n",
    "\n",
    "# Drop the original 'contract_start_date' and 'report_date' columns\n",
    "df = df.drop(['contract_start_date', 'report_date', 'customer_id'], axis='columns')\n",
    "#replace missing values in gross income and age with median of distribution\n",
    "count = df['gross_income'].isna().sum()\n",
    "df['gross_income'] = df['gross_income'].fillna(df['gross_income'].median())\n",
    "df.head(10)\n",
    "age_new = df.iloc[:,3]\n",
    "age_new[1020:1040]\n",
    "new = []\n",
    "for i in age_new:\n",
    "    if i != ' NA':\n",
    "        new.append(int(i))\n",
    "\n",
    "med = median(new)\n",
    "med\n",
    "age = []\n",
    "for i in age_new:\n",
    "    if i !=' NA':\n",
    "        age.append(int(i))\n",
    "    else:\n",
    "        age.append(med)\n",
    "age[1020:1040]\n",
    "df['age'] = age\n",
    "\n",
    "\n",
    "# Function to strip leading and trailing spaces from string columns\n",
    "def strip_spaces(column):\n",
    "    if column.dtype == 'object':  # Check if the column is of string type\n",
    "        return column.str.strip()  # Strip leading and trailing spaces\n",
    "    return column\n",
    "\n",
    "# Apply the strip_spaces function to all columns in the DataFrame\n",
    "df = df.apply(strip_spaces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode dataset, required to perform SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove customer index remove\n",
    "if 'customer_id' in df.columns:\n",
    "    df = df.drop(\"customer_id\", axis='columns')\n",
    "# Drop 'employee_index' if it exists in the DataFrame\n",
    "if 'employee_index' in df.columns:\n",
    "    df = df.drop('employee_index', axis='columns')\n",
    "# Remove leading/trailing whitespace of seniority_months column and convert to numeric\n",
    "df['seniority_months'] = pd.to_numeric(df['seniority_months'].str.strip(), errors='coerce')\n",
    "\n",
    "\n",
    "count = df['gross_income'].isna().sum()\n",
    "df['gross_income'] = df['gross_income'].fillna(df['gross_income'].median())\n",
    "\n",
    "##### One hot encode all non numeric columns\n",
    "# Specify the columns to check for NA values\n",
    "columns_with_na = [\n",
    "    'employee_index', 'country_residence', 'gender', \n",
    "    'customer_relation_type', 'residence_index', \n",
    "    'foreigner_index', 'join_channel', 'deceased_index', \n",
    "    'customer_segment'\n",
    "]\n",
    "\n",
    "# Filter columns that actually exist in the DataFrame\n",
    "existing_columns = [col for col in columns_with_na if col in df.columns]\n",
    "\n",
    "# Drop rows with NA values in the specified existing columns\n",
    "if existing_columns:\n",
    "    df = df.dropna(subset=existing_columns)\n",
    "    \n",
    "    # One-hot encode the specified existing columns (excluding 'province_name')\n",
    "    df = pd.get_dummies(df, columns=existing_columns, drop_first=True)\n",
    "\n",
    "# Remove rows where 'province_name' has NA values\n",
    "df = df.dropna(subset=['province_name'])\n",
    "## Changing province name to regions so less columns created via one hot encoding\n",
    "\n",
    "province = df['province_name']\n",
    "region = []\n",
    "for i in province:\n",
    "    if i in ['CIUDAD REAL', 'SALAMANCA','TOLEDO', 'SEGOVIA', 'MADRID', 'GUADALAJARA', 'ALBACETE', 'SORIA', 'CUENCA', 'AVILA']:\n",
    "        region.append(\"CENTRAL\")\n",
    "        \n",
    "    elif i in ['ALAVA', 'GIPUZKOA', 'PALENCIA', 'BURGOS', 'NAVARRA', 'CANTABRIA', 'BIZKAIA', 'RIOJA, LA', 'ZARAGOZA', 'TARRAGONA', 'LERIDA', 'HUESCA']:\n",
    "        region.append(\"NORTH\")\n",
    "\n",
    "    elif i in ['CADIZ','JAEN', 'SEVILLA', 'PALMAS, LAS', 'CORDOBA', 'GRANADA', 'SANTA CRUZ DE TENERIFE', 'MELILLA', 'CEUTA', 'MALAGA']:          \n",
    "        region.append(\"SOUTH\")\n",
    "    \n",
    "    elif i in ['VALENCIA', 'TERUEL', 'BALEARS, ILLES', 'CASTELLON', 'ALICANTE', 'MURCIA', 'ALMERIA', 'BARCELONA', 'GIRONA']:\n",
    "        region.append(\"EAST\")\n",
    "        \n",
    "    elif i in ['ZAMORA', 'CACERES', 'HUELVA', 'BADAJOZ', 'ASTURIAS', 'LEON', 'LUGO', 'CORUÑA, A', 'OURENSE', 'VALLADOLID', 'PONTEVEDRA']:\n",
    "        region.append(\"WEST\")\n",
    "    else:\n",
    "        continue\n",
    "df['province_name'] = region\n",
    "\n",
    "df = df.rename(columns={'province_name': 'region'})\n",
    "\n",
    "# One-hot encode the 'region' column\n",
    "if 'region' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['region'], drop_first=True)\n",
    "    \n",
    "# Convert True/False columns to 1/0\n",
    "bool_columns = df.select_dtypes(include=['bool']).columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SMOTE, this produces enough of each class to be used for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing for label: fixed_deposits\n",
      "Balancing for label: loan\n",
      "Balancing for label: credit_card_debit_card\n",
      "Balancing for label: account\n",
      "CSV file successfully relabelled and saved to cleaned_reco.csv\n"
     ]
    }
   ],
   "source": [
    "#select an equal amount of each class, and use SMOTE to balance the rest\n",
    "\n",
    "# Initialize the new balanced DataFrame\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Define the label columns to balance\n",
    "label_columns = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "\n",
    "# Set the maximum number of samples to take from any class\n",
    "max_samples = number_of_each_class  # Adjust as needed\n",
    "\n",
    "# Loop over each label column to balance it individually\n",
    "for label in label_columns:\n",
    "    print(f\"Balancing for label: {label}\")\n",
    "\n",
    "    # Separate the current label and the features\n",
    "    y = df[label]\n",
    "    X = df.drop(columns=label_columns)  # Keep all features but exclude other labels\n",
    "\n",
    "    # Prepare the data to balance the 0 and 1 classes for the current label\n",
    "    class_0 = df[df[label] == 0]\n",
    "    class_1 = df[df[label] == 1]\n",
    "\n",
    "    # Take a max of 'max_samples' or the available samples for each class\n",
    "    sampled_class_0 = class_0.sample(n=min(len(class_0), max_samples), random_state=42)\n",
    "    sampled_class_1 = class_1.sample(n=min(len(class_1), max_samples), random_state=42)\n",
    "\n",
    "    # Combine the samples to form the data for SMOTE\n",
    "    df_to_balance = pd.concat([sampled_class_0, sampled_class_1], ignore_index=True)\n",
    "\n",
    "    # Separate features and the label for SMOTE\n",
    "    X_balance = df_to_balance.drop(columns=label)\n",
    "    y_balance = df_to_balance[label]\n",
    "\n",
    "    # Apply SMOTE to balance the current label\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_balance, y_balance)\n",
    "\n",
    "    # Create a DataFrame from the resampled data\n",
    "    resampled_df = pd.DataFrame(X_resampled, columns=X_balance.columns)\n",
    "    resampled_df[label] = y_resampled  # Add the resampled label back\n",
    "\n",
    "    # Append the resampled data to the balanced_df\n",
    "    balanced_df = pd.concat([balanced_df, resampled_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the final balanced DataFrame\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True).head(number_of_each_class*2*len(label_columns))\n",
    "\n",
    "##moving the y columns to the front\n",
    "# Define the target columns to move to the front\n",
    "columns_to_move = ['fixed_deposits', 'loan', 'credit_card_debit_card', 'account']\n",
    "# Remove the target columns from the DataFrame\n",
    "remaining_columns = [col for col in balanced_df.columns if col not in columns_to_move]\n",
    "# Add the target columns back to the front\n",
    "balanced_df = balanced_df[columns_to_move + remaining_columns]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "balanced_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file successfully relabelled and saved to {output_file_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
