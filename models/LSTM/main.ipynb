{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yqEvvYuqBgaC"},"outputs":[],"source":["import os\n","import time\n","import datetime\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from tqdm import tqdm\n","\n","import importlib\n","\n","from model import LSTM\n","from dataset import Sliding, SlideDataset, yh_get_company_dat\n","from myutils import Loss_Meter, Config, plot_results\n","from optimizers import Optimizer\n","from criterions import Criterion\n","\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n","config = Config(\n","\tSEED = 0,\n","\n","\t# For dataset\n","\tWINDOW_SIZE = 15,\n","\n","\t# For dataloader\n","\tBATCH_SIZE = 4, WORKERS = 0, \n","\n","\t# For fixed number of steps, set Threshold = 0.\n","\tTHRESHOLD = 0e-04,\n","\n","\tEPOCHS = 30,\n","\tCHECKPOINT = 10,\n","\tLOG_DIR = os.path.join(\"logs\", \"{}\".format(now)).replace(\"\\\\\", \"/\"),\n","\n","\t# Model\n","\tINPUT_SIZE=1, HIDDEN_SIZE=7, NUM_LAYERS=1, DROP=0., \n","\n","\tOPTIMIZER = 'Adam',\n","\tLOSS = 'MSELoss',\n","\n","\t# For Adam\n","\tLR = 1e-03,\n","\tWEIGHT_DECAY = 1e-05,\n","\n","\t# For SGD\n","\tMOMENTUM = 9e-01, \n","\tDAMPENING = 0e-00,\n",")\n","\n","set_seed(config.SEED)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{},"source":["# Get Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = yh_get_company_dat(ticker='JPM')\n","df"]},{"cell_type":"markdown","metadata":{},"source":["# Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["slided = Sliding(df, X_period=config.WINDOW_SIZE)\n","\n","train_set, val_set, test_set = slided.get_Dataset()\n","\n","train_loader = DataLoader(train_set, batch_size = config.BATCH_SIZE, num_workers=config.WORKERS, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size = 128, num_workers=config.WORKERS, shuffle=False)\n","test_loader = DataLoader(test_set, batch_size = 128, num_workers=config.WORKERS, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not os.path.isdir(\"logs\"):\n","\tos.mkdir(\"logs\")\n","\n","best_model_path = os.path.join(config.LOG_DIR, 'checkpoint_best.pth').replace('\\\\', '/')\n","\n","model = LSTM(config,\n","\t\t\t# drop=0.1, \n","            act_layer = nn.LeakyReLU, \n","\t\t\tdevice=device)\n","\n","model.to(device, dtype=torch.float64)"]},{"cell_type":"markdown","metadata":{},"source":["# Optimizer, Loss, LR Scheduler, Gradient Scaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = Optimizer(config, model).get_Optim()\n","criterion = Criterion(config).get_Criterion()\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=1e-03)\n","g_scaler = torch.amp.GradScaler(device=device.type)"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Meters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_meter = Loss_Meter(loss_type ='train')\n","val_meter = Loss_Meter(loss_type ='val')\n","test_meter = Loss_Meter(loss_type ='test')"]},{"cell_type":"markdown","metadata":{},"source":["# Tensorboard Logging"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["writer = SummaryWriter(log_dir=config.LOG_DIR)\n","layout = {\"Loss Plot\" : {'loss': [\"Multiline\", ['train_loss', 'val_loss']]}}\n","writer.add_custom_scalars(layout)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model)\n","\n","epoch = 1\n","min_loss = 1e+05\n","val_loss = torch.tensor([1e+05])\n","train_loss = torch.tensor([1e+05])\n","loss_diff = 1e+05\n","\n","while True:\n","\tmodel.train()\n","\twith tqdm(train_loader, desc='Epoch {}'.format(str(epoch).zfill(2))) as tepoch:\n","\t\tfor i, (data, target) in enumerate(tepoch):\n","\t\t\t\n","\t\t\tdata, target = data.to(device), target.to(device)\n","\t\t\t\n","\t\t\tif config.OPTIMIZER == 'LBFGS':\n","\t\t\t\tdef closure():\n","\t\t\t\t\toptimizer.zero_grad()\n","\t\t\t\t\toutput = model(data)\n","\t\t\t\t\tloss = criterion(output, target)\n","\t\t\t\t\tloss.backward()\n","\t\t\t\t\ttrain_meter.update(loss.detach().cpu().item())\n","\t\t\t\t\treturn loss\n","\t\t\t\toptimizer.step(closure)\n","\t\t\telse:\n","\t\t\t\toptimizer.zero_grad()\n","\n","\t\t\t\toutput = model(data)\n","\t\t\t\t\n","\t\t\t\ttrain_loss = criterion(output, target)\n","\t\t\t\ttrain_meter.update(train_loss.detach().cpu().item())\n","\t\t\t\t\n","\t\t\t\tg_scaler.scale(train_loss).backward()\n","\t\t\t\t\n","\t\t\t\tg_scaler.unscale_(optimizer)\n","\t\t\t\tnn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","\t\t\t\tg_scaler.step(optimizer)\n","\t\t\t\tg_scaler.update()\n","\n","\n","\t\t\tif i % (len(tepoch)-1) == 0:\n","\t\t\t\t\n","\t\t\t\tmodel.eval()\n","\t\t\t\twith torch.no_grad():\n","\t\t\t\t\tfor val_data, val_target in val_loader:\n","\t\t\t\t\t\tval_data, val_target = val_data.to(device), val_target.to(device)\n","\t\t\t\t\t\tval_output = model(val_data)\n","\t\t\t\t\t\tval_loss = criterion(val_output, val_target)\n","\t\t\t\t\t\tval_meter.update(val_loss.detach().cpu().item())\n","\t\t\t\t\n","\t\t\t\tdiff = abs(train_meter.mean() - val_meter.mean())\n","\t\t\t\tif diff <= loss_diff: \n","\t\t\t\t\ttorch.save(model.state_dict(), best_model_path)\n","\t\t\t\t\t# min_loss = val_meter.mean()\n","\t\t\t\t\tloss_diff = diff\n","\n","\t\t\t\tif epoch % config.CHECKPOINT == 0:\n","\t\t\t\t\tcheckpoint_path = os.path.join(config.LOG_DIR,\n","\t\t\t\t\t\t'checkpoint_{}.pth'.format(str(epoch).zfill(2))).replace('\\\\','/')\n","\t\t\t\t\ttorch.save(model.state_dict(), checkpoint_path)\n","\t\t\t\t\t\n","\t\t\t\tmodel.train()\n","\t\t\t\tscheduler.step(val_loss)\n","\t\t\t\n","\t\t\ttepoch.set_postfix(\n","\t\t\t\ttrain_loss = \"{:.10e}\".format(train_meter.mean()),\n","\t\t\t\tval_loss = \"{:.5e}\".format(val_meter.mean())\n","\t\t\t)\n","\twriter.add_scalar('train_loss'.format(epoch), train_meter.mean(), epoch)\n","\twriter.add_scalar('val_loss'.format(epoch), val_meter.mean(), epoch)\n","\n","\tif ((not config.THRESHOLD) and epoch == config.EPOCHS) or (loss_diff <= config.THRESHOLD):\n","\t\tbreak\n","\n","\tepoch += 1 \n","\ttorch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model.load_state_dict(torch.load(best_model_path))\n","model.eval()\n","with torch.no_grad():\n","\twith tqdm(test_loader, desc='Test {}'.format(str(epoch))) as tepoch:\n","\t\tfor test_data, test_target in tepoch:\n","\t\t\ttest_data, test_target = test_data.to(device), test_target.to(device)\n","\t\t\t\n","\t\t\ttest_output = model(test_data)\n","\t\t\ttest_loss = criterion(test_output, test_target)\n","\t\t\ttest_meter.update(test_loss.detach().cpu().item())\n","\t\t\t\n","\t\t\ttepoch.set_postfix(test_loss = \"{:.5e}\".format(test_meter.mean()))"]},{"cell_type":"markdown","metadata":{},"source":["# Performance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_preds = np.concatenate((\n","\tmodel(train_set.X.to(device)).detach().cpu().numpy().flatten(), \n","\tmodel(val_set.X.to(device)).detach().cpu().numpy().flatten(),\n","\tmodel(test_set.X.to(device)).detach().cpu().numpy().flatten()\n","))\n","\n","y_preds = slided.unscale(config, y_preds)\n","# df.dropna(inplace=True)\n","# df['Predictions'] = np.pad(y_preds, (0, len(df) - len(y_preds)))\n","df['Predictions'] = y_preds\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_results(df, train_set.__len__(), val_set.__len__(), test_set.__len__())"]}],"metadata":{"colab":{"provenance":[{"file_id":"1y5_ztdcldI-iKVuMPGUfmrwZk1mIJAEv","timestamp":1713971052447},{"file_id":"1y5_ztdcldI-iKVuMPGUfmrwZk1mIJAEv","timestamp":1713970764118}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
