{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Generate data for customer response regarding a banking experience, make sure to have parts describing convenience, speed and informative, then label which parts are convenient, speed and informative, format this in a jsonl, with keys = \"key\", \"transcript\", \"convenient\", \"speed\", \"informative\". Make sure to label the tokens with phrases present in the transcript. Key is index.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. API Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## microsoft/phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```jsonl\n",
      "{\n",
      "  \"key\": \"1\",\n",
      "  \"transcript\": \"I found Bank X super convenient because they are available outside regular banking hours.\",\n",
      "  \"convenient\": \"super convenient, available outside regular banking hours\",\n",
      "  \"speed\": null,\n",
      "  \"informative\": null\n",
      "}\n",
      "```\n",
      "\n",
      "```jsonl\n",
      "{\n",
      "  \"key\": \"2\",\n",
      "  \"transcript\": \"The speed of the transactions at Bank Y was incredible, I completed all my banking needs in just a few minutes.\",\n",
      "  \"convenient\": null,\n",
      "  \"speed\": \"The speed, incredible, all my banking needs in just a few minutes\",\n",
      "  \"informative\": null\n",
      "}\n",
      "```\n",
      "\n",
      "```jsonl\n",
      "{\n",
      "  \"key\": \"3\",\n",
      "  \"transcript\": \"I received an email explaining my account fees in detail, making it very informative and easy to understand.\",\n",
      "  \"convenient\": null,\n",
      "  \"speed\": null,\n",
      "  \"informative\": \"an email explaining, my account fees in detail, easy to understand\"\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "The JSONL format documents what parts of the transcript have references to convenience, speed, and informative aspects of the banking service. Each entry includes a key corresponding to an index, a transcript section highlighting relevant parts, and labeled labels indicating convenience, speed, and informative qualities, as applicable."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=\"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\")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmodel=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "\tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## google/gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "# client = InferenceClient(api_key=\"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\")\n",
    "\n",
    "# for message in client.chat_completion(\n",
    "# \tmodel=\"google/gemma-2-2b-it\",\n",
    "# \tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "# \tmax_tokens=500,\n",
    "# \tstream=True,\n",
    "# ):\n",
    "#     print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'generate data for customer response regarding a banking experience, make sure to have parts describing convenience, speed and informative, then label which parts are convenient, speed and informative, format this in a jsonl, with keys = \"key\", \"transcript\", \"convenient\", \"speed\", \"informative\". Make sure to label the tokens with phrases present in the transcript.'},\n",
       " {'generated_text': 'generate data for customer response regarding a banking experience, make sure to have parts describing convenience, speed and informative, then label which parts are convenient, speed and informative, format this in a jsonl, with keys = \"key\", \"transcript\", \"convenient\", \"speed\", \"informative\". Make sure to label the tokens with phrases present in the transcript.\\n\\nExample 2\\n\\n1 â€“ To make it easier to read\\n\\nLet\\'s create a table and let\\'s'},\n",
       " {'generated_text': 'generate data for customer response regarding a banking experience, make sure to have parts describing convenience, speed and informative, then label which parts are convenient, speed and informative, format this in a jsonl, with keys = \"key\", \"transcript\", \"convenient\", \"speed\", \"informative\". Make sure to label the tokens with phrases present in the transcript.\\n\\nYou should get a couple of responses from the developers. Most likely you won\\'t get any kind of data after'},\n",
       " {'generated_text': 'generate data for customer response regarding a banking experience, make sure to have parts describing convenience, speed and informative, then label which parts are convenient, speed and informative, format this in a jsonl, with keys = \"key\", \"transcript\", \"convenient\", \"speed\", \"informative\". Make sure to label the tokens with phrases present in the transcript.\\n\\nNote that some of the time, people don\\'t understand the syntax for adding tokens to statements, they might interpret'},\n",
       " {'generated_text': 'generate data for customer response regarding a banking experience, make sure to have parts describing convenience, speed and informative, then label which parts are convenient, speed and informative, format this in a jsonl, with keys = \"key\", \"transcript\", \"convenient\", \"speed\", \"informative\". Make sure to label the tokens with phrases present in the transcript. Include the first few phrases in the beginning of the tokens. This example will explain what the tokens represent.\\n\\nLet'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(prompt, max_length=100, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_gpt2(prompt):\n",
    "    input_ids = gpt2_tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    output = gpt2_model.generate(input_ids, max_length=1024, num_return_sequences=1)\n",
    "\n",
    "    generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gpt2_output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mgenerate_gpt2\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_gpt2\u001b[39m(prompt):\n\u001b[0;32m      7\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m gpt2_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m gpt2_tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_text[\u001b[38;5;28mlen\u001b[39m(prompt):]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\transformers\\generation\\utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2025\u001b[0m         input_ids,\n\u001b[0;32m   2026\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2027\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   2028\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2029\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2030\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2031\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2032\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2033\u001b[0m     )\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\transformers\\generation\\utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1337\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1335\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1337\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\torchdev\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpt2_output = generate_gpt2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your portfolio.\n",
      "\n",
      "What is your portfolio?\n",
      "\n",
      "My portfolio is a portfolio of stocks and bonds. I have a portfolio of $100,000. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000 in stocks and bonds. I have a portfolio of $100,000\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
